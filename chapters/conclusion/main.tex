\graphicspath{{chapters/\chap/figures/}}
\makeatletter
\def\input@path{{chapters/\chap/figures/}}
\makeatother

\chapter{Conclusion and future directions}\label{sec:\chap}

%This thesis presented three numerical methods for solving elliptic PDEs, all focused on delivering high-order accuracy with low computational complexity. %The development of such solvers requires a careful balance between algorithmic cost and mathematical fidelity.
%This thesis presented three fast methods for the high-order accurate numerical solution of elliptic PDEs.
%With the end of Moore's law present, speedups in computation time must come from smarter algorithms, either through increased parallelism or decreased computational complexity~\cite{NASA_Vision_2030}. Numerical analysis is well poised to tackle the latter.

This thesis presented three fast methods for the high-order accurate numerical solution of elliptic PDEs. \cref{sec:fast-spectral-poisson} developed optimal complexity Poisson solvers that achieve spectral accuracy by exploiting the separated spectra of differential operators discretized by ultraspherical polynomials. There, we employed ADI as a direct solver to solve Sylvester matrix equations for Poisson problems on the rectangle, cylinder, sphere, and cube.

To move beyond simple geometries, we developed the ultraspherical spectral element method for solving elliptic PDEs with high-order polynomials on unstructured meshes in \cref{sec:ultraSEM}. Using a hierarchical analogue of Gaussian elimination for domain decomposition, we created a fast direct solver for our SEM that computes the solution in $\mathcal{O}(p^4/h^3)$ operations and allows for fast repeated solves, enabling the acceleration of implicit time-steppers. We packaged the method into the \ultraSEM software, which is designed for fast spectral element computation and $hp$-adaptivity with polynomials of very high degree.

In \cref{sec:multigrid-ldg}, we presented an $hp$-multigrid method for LDG discretizations of elliptic problems that is based on coarsening the discrete gradient and divergence operators from the flux formulation. We showed that coarsening fine-grid operators in this way results in a method that is equivalent to pure geometric multigrid, but avoids the need to compute quantities associated with coarse meshes, such as lifting operators and quadrature rules. Convergence factors were shown to be nearly independent of both mesh size $h$ and polynomial order $p$ for the demonstrated test problems on uniform Cartesian grids, adaptively refined meshes, and implicitly defined meshes on complex geometries. %Whereas traditional Galerkin operator coarsening applied to the primal formulation exhibits poor multigrid performance, operator coarsening applied to the flux formulation performs well---convergence factors are nearly independent of both mesh size $h$ and polynomial order $p$ for the demonstrated test problems on uniform Cartesian grids, adaptively refined meshes, and implicitly defined meshes on complex geometries.

\vspace{0.5em}
\begin{center}$ {\ast}\;{\ast}\;{\ast} $\end{center}
\vspace{0.5em}

\noindent We now briefly describe some potential extensions of the work in thesis.

\section*{Spectral element methods}

The ultraspherical spectral element method allows for sparse, very high-order discretizations of elliptic PDEs on two-dimensional meshes to be solved efficiently. Though heavily optimized, our \ultraSEM software---which implements the element method in MATLAB---is far from ready for large-scale use. In order to realize the potential of $hp$-adaptivity with large $p$ in high-performance settings, significant advancements to \ultraSEM are necessary:

\vspace{0.2em}
\begin{itemize}

\item \textbf{Iterative methods and robust preconditioners.} Iterative methods are typically avoided in the global spectral methods community due to their sensitivity to conditioning, despite the fact that many spectral discretizations possess fast, FFT-based matrix-vector products. Recently, Krylov methods based on operator-function products have performed well for solving Poisson problems on the square with careful preconditioning~\cite{Gilles_19_01}. For problems on meshed geometries or for more complex elliptic PDEs, it may be possible to build a robust preconditioner based on \ultraSEM. Using a continuous analogue of the singular value decomposition for partial differential operators, near-optimal Kronecker product preconditioners may be constructed at the continuous level, which can be inverted by \ultraSEM and applied in an iterative method. Combine \ultraSEM with Newton iteration may also allow \ultraSEM to solve nonlinear problems.

\item \textbf{Stability.} It is unknown if the merge step in the HPS scheme is stable. That is, does the merged solution operator $S_\Gamma$ always exist? And if so, how does the conditioning of the merged solution operator $S_\Gamma$ depend on the conditioning of its children? While preliminary experiments indicate that the conditioning of $S_\Gamma$ does not depend on the merge order of its children, it is unclear as to the role that merge order plays in the HPS scheme.

\item \textbf{Optimal complexity in $h$ and $p$.} In $d$ dimensions, the number of degrees of freedom in a (uniform) spectral element method scales as $N \approx (p/h)^d$. Thus, a spectral element method with optimal computational complexity would scale as $\mathcal{O}(p^2/h^2)$ in two dimensions or $\mathcal{O}(p^3/h^3)$ in three dimensions. Currently, the ultraspherical spectral element method has an overall complexity of $\mathcal{O}(p^4/h^3)$ in two dimensions, and can be extended to three-dimensional geometries with a complexity of $\mathcal{O}(p^6/h^6)$. It is an open question as to whether an optimal complexity SEM exists in two or three dimensions. It is possible that further structure in the almost block-banded matrices created by the ultraspherical spectral method may be exploited using low-rank compression techniques, to further reduce the complexity by a factor of $p$.

%, though achieving a competitive computational complexity is not straightforward. The use of even moderately large $p$ (e.g. $p = 30$) in 3D is prohibitively expensive for current spectral element methods. Merging 3D domains using the hierarchical Poincar\'{e}--Steklov scheme has been studied~\cite{Hao_16_01}, but more computational advancements are required in order to reduce the computational burden of the large $p$ regime. Na\"{i}vely, in three dimensions the cost of solving for $\mathcal{O}(p^3/h^3)$ degrees of freedom using the hierarchical Poincar\'{e}--Steklov scheme is $\mathcal{O}(p^9/h^6)$. The sparse discretizations in the coefficient-based approach of the ultraspherical spectral method allow the computational cost to be reduced to $\mathcal{O}(p^6/h^6)$, putting $h$ and $p$ on equal footing where $p=30$ is attainable. To further reduce the algorithmic complexity in 3D, low rank compression techniques may be used to more efficiently pass solution information between neighboring elements.

%\item \textbf{Three dimensions.} The ultraspherical spectral element method can be extended to 3D geometries, though achieving a competitive computational complexity is not straightforward. The use of even moderately large $p$ (e.g. $p = 30$) in 3D is prohibitively expensive for current spectral element methods. Merging 3D domains using the hierarchical Poincar\'{e}--Steklov scheme has been studied~\cite{Hao_16_01}, but more computational advancements are required in order to reduce the computational burden of the large $p$ regime. Na\"{i}vely, in three dimensions the cost of solving for $\mathcal{O}(p^3/h^3)$ degrees of freedom using the hierarchical Poincar\'{e}--Steklov scheme is $\mathcal{O}(p^9/h^6)$. The sparse discretizations in the coefficient-based approach of the ultraspherical spectral method allow the computational cost to be reduced to $\mathcal{O}(p^6/h^6)$, putting $h$ and $p$ on equal footing where $p=30$ is attainable. To further reduce the algorithmic complexity in 3D, low rank compression techniques may be used to more efficiently pass solution information between neighboring elements.

%\item \textbf{Parallelization.} The domain decomposition approach of the ultraspherical spectral element method lends itself to parallelization. Local solution operations on each element decouple, allowing operators on elements or groups of elements to be computed independently and thus parallelized. Merging elements together can be decomposed into a tree of operations whose branches can be computed in parallel as well. Targeting the spectral element method for many-core and many-node machines will allow for large-scale industrial simulations with high polynomial degree.

\end{itemize}

We are hoping to apply \ultraSEM to problems such as advection-dominated fluid flow and high-frequency scattering, where low-order methods can artificially pollute the solution.

%These advancements would allow \ultraSEM to be readily applied to problems such as advection-dominated fluid flow and high-frequency scattering, where low-order methods can artificially pollute the solution.

\section*{Discontinuous Galerkin methods}

Designing geometric multigrid methods for LDG discretizations of elliptic problems requires careful treatment of the discrete Laplacian operator to achieve good multigrid performance. A number of interesting questions arise from generalizing operator-coarsening approaches:
%This phenomenon raises numerous questions about the generality of operator-coarsening approaches:

\vspace{0.2em}
\begin{itemize}

\item \textbf{Beyond LDG.} Though most of the analysis in \cref{sec:multigrid-ldg} focused on the LDG method, we believe that the essential observation applies to other forms of DG discretization of elliptic problems, particularly those in which lifting operators enter the numerical flux for $\vec q$. A more thorough analysis for other DG methods and more general choices of numerical fluxes would be required to determine whether the multigrid method described here extends to other methods, such as CDG or HDG. Similarly, though we have employed equal-order elements in this work, i.e., polynomials of the same degree for both $u$ and $\vec q$, operator-coarsening for mixed-order elements~\cite{Brezzi_05_01} would be an interesting topic for future investigation.

\item \textbf{Algebraic multigrid.} Algebraic multigrid (AMG) methods build a hierarchy of coarse operators directly from a given matrix, by identifying fine and coarse unknowns and constructing interpolation and restriction operators that preserve algebraically smooth error. For finite element problems on unstructured and adaptively-refined meshes, and for problems with anisotropic variable coefficients, the algebraic multigrid method can be an efficient solver when geometric coarsening is cumbersome or impossible. The idea of coarsening the divergence and gradient operators separately may be useful for AMG methods, which currently treat the discrete Laplacian operator in its entirety as a black box. Indeed, black-box AMG algorithms applied to LDG discretizations appear to struggle~\cite{Olson_11_01}; perhaps applying AMG separately to the divergence and gradient operators in the flux formulation may yield better results. The AMG process may identify different coarse-grid unknowns for each operator, and so a unified set of coarse-grid unknowns would need to be constructed in order to compose the divergence and gradient operators in a consistent manner.

\item \textbf{Unstructured meshes.} \cref{sec:multigrid-ldg} considered structured meshes (Cartesian, quadtree, and octree meshes) as well as semi-unstructured, nonconforming, implicitly defined meshes that result from cell merging procedures (see \cref{fig:multigrid-ldg:lemniscate,fig:multigrid-ldg:interface}); applying the multigrid ideas presented here to problems involving more general unstructured meshes is an area for future investigation. In this setting, it may be worthwhile to consider different types of relaxation methods owing to their critical role in the overall efficacy of a multigrid method. For example, additive Schwarz smoothers have been shown effective on non-nested polygonal meshes resulting from agglomeration procedures~\cite{Antonietti_19_01}; these smoothers could be studied in the flux coarsening context as well.

\end{itemize}

We are also investigating the development of high-order DG methods for Eulerian solid mechanics. Recent work on the reference map technique~\cite{Kamrin_09_01,Kamrin_12_01,Levin_11_01} has demonstrated the success of low-order finite difference methods for the simulation of finite-strain elasticity in an Eulerian reference frame. Such a formulation allows for fluid--solid coupling to be handled naturally, as both fluid and solid can be tracked on the same fixed mesh~\cite{Valkov_15_01,Rycroft_20_01,Dunne_2013,Teng_16_01,Jain_19_01}. However, the dissipation inherent in finite difference schemes can make accurate simulation challenging, e.g., in rapidly vibrating objects with low physical dissipation such as mechanical resonators~\cite{Govindjee_12_01} and tuning forks~\cite{Froehle_14_01}, or in objects with large spatial variation in elastic moduli such as the Earth's interior. A DG formulation of the reference map technique would allow for the high-order simulation of large-deformation solids with minimal energy loss, and could naturally be coupled to a high-order accurate method for fluid simulation to yield a fully Eulerian, high-order method for fluid--solid interaction.

%As higher-order accurate techniques are utilized by more researchers in HPC environments, computational efficiency is critical. The NASA CFD Vision 2030 sees potential windfall in the development of optimal PDE solvers: ``Because solver optimality is an asymptotic property, as larger simulations are attempted, the potential benefits of better solvers grow exponentially, possibly delivering orders of magnitude improvement by the exascale computing timeframe''~\cite{NASA_Vision_2030}. This sentiment is echoed by the applied mathematics community: ``As problems scale in size and memory to address increasing needs for fidelity and resolution in grand-challenge simulations, the computational complexity must scale as close to linearly in the problem size as possible''~\cite{CSE_Outlook_SIAM}. With the end of Moore's law present, speedups in computation time must come from smarter algorithms, either through increased parallelism or decreased computational complexity. Numerical analysis is well poised to tackle the latter.
